{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Toxic Chat Classification Using NLP\n",
    "\n",
    "## Introduction\n",
    "In this project, we will be using Natural Language Processing (NLP) to classify toxic comments in a chat. The dataset we will be using is the \n",
    "1. ) [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) dataset from Kaggle. The dataset consists of comments from Wikipediaâ€™s talk page edits. The comments are labeled as toxic, severe toxic, obscene, threat, insult, and identity hate. The goal of this project is to build a model that can classify the comments into these categories.\n",
    "2. ) [Sensai] is a toxic chat dataset consists of live chats from Virtual YouTubers' live streams\n",
    "   curl -L -o ~/Downloads/archive.zip\\ https://www.kaggle.com/api/v1/datasets/download/uetchy/sensai\n",
    "3. ) [The Toxicity Dataset] This repo contains 500 toxic and 500 non-toxic comments from a variety of popular social media platforms. https://github.com/surge-ai/toxicity\n",
    "\n",
    "## Members - Cluster D Table 25\n",
    "1. ) HTUN HTET MYAT\n",
    "2. )\n",
    "3. )\n",
    "4. )\n",
    "5. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/blake/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/blake/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import Statements \n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import requests\n",
    "import nltk\n",
    "import string\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset, Dataset\n",
    "import sentencepiece\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Functions to clean the NLP Dataset, remove stopwords, lemmatize, and stem the words\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "    \n",
    "def tagged_lemma(string):\n",
    "    \n",
    "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(string))\n",
    "\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "\n",
    "    lemmatized_sentence = []\n",
    "\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:       \n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "\n",
    "def clean_string(text, stem=\"None\"):\n",
    "\n",
    "    # Final String to return\n",
    "    final_string = \"\"\n",
    "\n",
    "    # Make the text to be lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove line breaks\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "\n",
    "    # Remove punctuations\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Remove stop words\n",
    "    text = text.split()\n",
    "    useless_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    text_filtered = [word for word in text if not word in useless_words]\n",
    "\n",
    "    # Remove numbers\n",
    "    text_filtered = [re.sub(r'\\w*\\d\\w*', '', w) for w in text_filtered]\n",
    "\n",
    "    # Stem or Lemmatize\n",
    "    if stem == 'Stem':\n",
    "        stemmer = PorterStemmer() \n",
    "        text_stemmed = [stemmer.stem(y) for y in text_filtered]\n",
    "    elif stem == 'Lem':\n",
    "        lem = WordNetLemmatizer()\n",
    "        text_stemmed = [lem.lemmatize(y) for y in text_filtered]\n",
    "    else:\n",
    "        text_stemmed = text_filtered\n",
    "\n",
    "    final_string = ' '.join(text_stemmed)\n",
    "\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate'],\n",
      "      dtype='object')\n",
      "Index(['body', 'label'], dtype='object')\n",
      "Index(['text', 'is_toxic'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Start with Cleaning the Data of Kaggle Toxic Comment Dataset\n",
    "\n",
    "df_kaggle = pd.read_csv('./data/kaggle-toxic-comment-challange/kaggle_train.csv')\n",
    "df_sensai = pd.concat([pd.read_parquet(x) for x in glob.glob('./data/sensai/*.parquet')], ignore_index=True)\n",
    "df_surge = pd.read_csv('./data/surge-ai-toxicity-repo/toxicity_en.csv')\n",
    "\n",
    "## Pretty print alll the dataframes columns\n",
    "print(df_kaggle.columns)\n",
    "print(df_sensai.columns)\n",
    "print(df_surge.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the datasets and put them in the cleaned column\n",
    "df_kaggle['comment_text_cleaned'] = df_kaggle['comment_text'].apply(lambda x: clean_string(x, stem=\"Lem\"))\n",
    "df_sensai['body_cleaned'] = df_sensai['body'].apply(lambda x: clean_string(x, stem=\"Lem\"))\n",
    "df_surge['text_cleaned'] = df_surge['text'].apply(lambda x: clean_string(x, stem=\"Lem\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframes with the cleaned data in CSV for backup\n",
    "df_kaggle.to_csv('./data/kaggle_train_cleaned.csv', index=False)\n",
    "df_sensai.to_csv('./data/sensai_cleaned.csv', index=False)\n",
    "df_surge.to_csv('./data/toxicity_en_cleaned.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
